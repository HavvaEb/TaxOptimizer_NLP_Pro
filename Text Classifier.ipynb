{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0171d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import glob \n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from spacy import displacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "from spacy.lang.nl.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from spacy.lang.nl import Dutch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c929f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a medium sized dutch language model in spacy\n",
    "nlp = spacy.load('nl_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8306cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ref for attempting summarization https://www.numpyninja.com/post/text-summarization-through-use-of-spacy-library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dd0085",
   "metadata": {},
   "outputs": [],
   "source": [
    "myfile = open(\"TaxRelatedFile.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af3eae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = myfile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf45a6e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675e29b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cf0c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f069fb12",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0876edb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take a look at how many words are in the document\n",
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260ee49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look document-level attributes\n",
    "dir(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b373cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5eb2f4",
   "metadata": {},
   "source": [
    "Using spaCy's built-in visualizer to detect named entities in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e319c9f5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2775db",
   "metadata": {},
   "source": [
    "Look up at label LAW, some tokens related to taxes were identified by spaCy with this label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b667ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spacy.explain(\"LAW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed2fd32",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68777f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = str(\" \".join([i.lemma_ for i in doc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e80e202",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc = nlp(review)\n",
    "spacy.displacy.render(doc, style='ent',jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d085a920",
   "metadata": {},
   "source": [
    "### Parts of Speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33fe16c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in nlp(review):\n",
    "    print(i, \"=>\", i.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391cab02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34023291",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e240516",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import puntuaction marks from string and also add additional next line tag in it\n",
    "punctuation=punctuation+ '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5720ded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the words from the sentence:\n",
    "tokens=[token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f928f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating word frequencies from the text after removing stopwords and puntuactions:\n",
    "\n",
    "stopwords = list(STOP_WORDS)\n",
    "\n",
    "word_frequencies={}\n",
    "for word in doc:\n",
    "    if word.text.lower() not in stopwords:\n",
    "        if word.text.lower() not in punctuation:\n",
    "            if word.text not in word_frequencies.keys():\n",
    "                word_frequencies[word.text] = 1\n",
    "            else:\n",
    "                word_frequencies[word.text] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28127768",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Print and see word frequencies to know important words.\n",
    "print(word_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035b8ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the maximum frequency and divide it by all frequencies to get normalized word frequencies.\n",
    "max_frequency=max(word_frequencies.values())\n",
    "for word in word_frequencies.keys():\n",
    "    word_frequencies[word]=word_frequencies[word]/max_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47f9c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing normalized word frequencies:\n",
    "#Printing in descending order \n",
    "\n",
    "#print(word_frequencies)\n",
    "w_sorted_keys = sorted(word_frequencies, key=word_frequencies.get, reverse=True)\n",
    "for w in w_sorted_keys:\n",
    "    print(w, word_frequencies[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37586f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get sentence tokens \n",
    "sentence_tokens= [sent for sent in doc.sents]\n",
    "print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46cb326",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the most important sentences by adding the word frequencies in each sentence.\n",
    "sentence_scores = {}\n",
    "for sent in sentence_tokens:\n",
    "    for word in sent:\n",
    "        if word.text.lower() in word_frequencies.keys():\n",
    "            if sent not in sentence_scores.keys():                            \n",
    "             sentence_scores[sent]=word_frequencies[word.text.lower()]\n",
    "            else:\n",
    "             sentence_scores[sent]+=word_frequencies[word.text.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8838d255",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Print sentence scores\n",
    "sentence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a5a05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#From headhq import nlargest and calculate  30% of text with maximum score.\n",
    "from heapq import nlargest\n",
    "select_length=int(len(sentence_tokens)*0.3)\n",
    "select_length\n",
    "summary=nlargest(select_length, sentence_scores,key=sentence_scores.get)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8afabd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Get the summary of text\n",
    "final_summary=[word.text for word in summary]\n",
    "final_summary\n",
    "summary=''.join(final_summary)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d77603",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22bcb0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all the texts together\n",
    "# import glob \n",
    "\n",
    "# path = 'text_nl/*.txt'\n",
    "\n",
    "# all_texts=\"\"\n",
    "\n",
    "# for file in glob.glob(path):\n",
    "#     with open(file, encoding='utf-8', errors='ignore') as file_in:\n",
    "#         text = file_in.read()\n",
    "#         all_texts+=text\n",
    "#         lines = text.split('\\n')\n",
    "#         for line in lines:\n",
    "#             line = nlp(line)\n",
    "#             for token in line:\n",
    "#                 print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15757d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7f2b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs = list(nlp.pipe(all_texts), n_process=4)\n",
    "# docs = nlp.pipe(all_texts, n_process=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f39e08e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dc1cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# all_laws = []\n",
    "\n",
    "# for d in docs:\n",
    "#     laws = [ent.text for ent in d.ents if ent.label_ == \"LAW\"]\n",
    "#     all_laws.extend(laws)\n",
    "\n",
    "# Counter(all_laws).most_common(2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0539f2",
   "metadata": {},
   "source": [
    "### Creating a dataframe from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75ec68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all the texts together\n",
    "def createDF(path):\n",
    "    \"\"\"\n",
    "    This function receives a path where files and merge the files into a dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    path = path\n",
    "    dataframes = []\n",
    "    df = pd.DataFrame(columns = [\"article_content\"])\n",
    "\n",
    "    for file in glob.glob(path):\n",
    "        with open(file, encoding='utf-8', errors='ignore') as file_in:\n",
    "            dataframe = file_in.read().replace('\\n', '')\n",
    "            dataframes.append(dataframe)\n",
    "    to_append = dataframes\n",
    "    my_series = pd.Series(to_append)\n",
    "    df[\"article_content\"]= my_series\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6a9f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = createDF('clean_text_nl/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd917b54",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55474227",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e9f8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[[6]].to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a746440f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ebc1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parser for content\n",
    "# parser = Dutch()\n",
    "# punctuations = punctuation\n",
    "# stopwords = list(STOP_WORDS)\n",
    "# def spacy_tokenizer(sentence):\n",
    "#     mytokens = parser(sentence)\n",
    "#     mytokens = [word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "#     print(mytokens)\n",
    "#     mytokens = [word for word in mytokens if word not in stopwords and word not in punctuations ]\n",
    "#     mytokens = \" \".join([i for i in mytokens])\n",
    "#     return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ec306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ref: https://stackoverflow.com/questions/45605946/how-to-do-text-pre-processing-using-spacy\n",
    "stopwords = list(STOP_WORDS)\n",
    "punctuations = punctuation\n",
    "def normalize(comment, lowercase, remove_stopwords):\n",
    "    \"\"\"\n",
    "    This function is used to normalize the text, remove stopwords and punctuations\n",
    "    \n",
    "    \"\"\"\n",
    "    if lowercase:\n",
    "        comment = comment.lower()\n",
    "    comment = nlp(comment)\n",
    "    lemmatized = list()\n",
    "    for word in comment:\n",
    "        lemma = word.lemma_.strip()\n",
    "        if lemma:\n",
    "            if not remove_stopwords or (remove_stopwords and lemma not in stopwords and lemma not in punctuations):\n",
    "                lemmatized.append(lemma)\n",
    "    return \" \".join(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2054bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "df[\"processed_content\"] = df[\"article_content\"].progress_apply(normalize, lowercase=True, remove_stopwords=True)\n",
    "#df[\"processed_content\"] = df[\"article_content\"].progress_apply(spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05626ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5ca024",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"processed_content\"][6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555b192d",
   "metadata": {},
   "source": [
    "### Topic-modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6266017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a vectorizer\n",
    "vectorizer = CountVectorizer(min_df=5, max_df=0.9, stop_words=stopwords, lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\n",
    "data_vectorized = vectorizer.fit_transform(df[\"processed_content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c0bc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using tfidf vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words=stopwords)\n",
    "tfidf = tfidf_vectorizer.fit_transform(df[\"processed_content\"])\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fca40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94b8987",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Latent Dirichlet Allocation Model\n",
    "lda = LatentDirichletAllocation(n_components=NUM_TOPICS, max_iter=10, learning_method='online',verbose=True)\n",
    "data_lda = lda.fit_transform(data_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4c0075",
   "metadata": {},
   "source": [
    "### Using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff9320a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for printing keywords for each topic\n",
    "def selected_topics(model, vectorizer, top_n=10):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print([(vectorizer.get_feature_names_out()[i], topic[i])\n",
    "                        for i in topic.argsort()[:-top_n - 1:-1]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4979f8c7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Keywords for topics clustered by Latent Dirichlet Allocation\n",
    "print(\"LDA Model:\")\n",
    "selected_topics(lda, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca40d5d",
   "metadata": {},
   "source": [
    "### Using NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf59b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Negative Matrix Factorization Model\n",
    "nmf = NMF(n_components=NUM_TOPICS)\n",
    "data_nmf = nmf.fit_transform(data_vectorized) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe68cb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keywords for topics clustered by Non-Negative Matrix Factorization\n",
    "\n",
    "print(\"NMF Model:\")\n",
    "selected_topics(nmf, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2de393",
   "metadata": {},
   "source": [
    "### Using Latent Semantic Indexing Model using Truncated SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455ca221",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi = TruncatedSVD(n_components=NUM_TOPICS)\n",
    "data_lsi = lsi.fit_transform(data_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee276b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keywords for topics clustered by Latent Semantic Indexing\n",
    "print(\"LSI Model:\")\n",
    "selected_topics(lsi, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d84489",
   "metadata": {},
   "source": [
    "### Visualizing LDA results with pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b77137",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "dash = pyLDAvis.sklearn.prepare(lda, data_vectorized, vectorizer, mds='tsne')\n",
    "dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d904298",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ref https://ratulesrar3.github.io/sotu-approval-analysis/\n",
    "def show_wordcloud(data, title = None):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='white',\n",
    "        stopwords=stopwords,\n",
    "        max_words=200,\n",
    "        max_font_size=40, \n",
    "        scale=3,\n",
    "        random_state=1 # chosen at random by flipping a coin; it was heads\n",
    "    ).generate(str(data))\n",
    "\n",
    "    fig = plt.figure(1, figsize=(12, 12))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5855f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        data = ' '.join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "        title = 'Topic {}'.format(topic_idx+1)\n",
    "        show_wordcloud(data, title)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0f41ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "no_top_words = 13\n",
    "display_topics(lda, tfidf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5548bec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6074df6fe51e2356874fb13e5f664d89034541b8aea5d05983084b4cbb6e7b40"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
